# -*- coding: utf-8 -*-
"""VisionGuardian.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UqhUkxgPFh8x4ghhPJqcU8E1T3j4uW4c

# Importing packages
"""

# Basic packages
import pandas as pd
import numpy as np
import re
import collections
import matplotlib.pyplot as plt
from pathlib import Path

# Packages for data preparation
from sklearn.model_selection import train_test_split
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from keras.preprocessing.image import ImageDataGenerator
from keras.optimizers import Adam

import nltk

# Packages for modeling
import tensorflow as tf
from tensorflow.keras import models
from tensorflow.keras import layers
from tensorflow.keras import regularizers

import cv2
import os

"""# Load the Dataset

"""

from google.colab import drive
drive.mount('/content/drive')

from google.colab import files
uploaded = files.upload()

for dirname, _, filenames in os.walk('/content/drive/MyDrive/Cataract'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

img_height, img_width = 32, 32
batch_size = 16

train_ds = tf.keras.utils.image_dataset_from_directory(
    "/content/drive/MyDrive/Cataract/train",
    image_size = (img_height, img_width),
    batch_size = batch_size
)
test_ds = tf.keras.utils.image_dataset_from_directory(
    "/content/drive/MyDrive/Cataract/test",
    image_size = (img_height, img_width),
    batch_size = batch_size
)

model = tf.keras.Sequential(
    [
    #  tf.keras.layers.Rescaling(1./255),
     tf.keras.layers.Conv2D(32, 3, activation="relu"),
     tf.keras.layers.MaxPooling2D(),
     tf.keras.layers.Conv2D(32, 3, activation="relu"),
     tf.keras.layers.MaxPooling2D(),
     tf.keras.layers.Conv2D(32, 3, activation="relu"),
     tf.keras.layers.MaxPooling2D(),
     tf.keras.layers.Flatten(),
     tf.keras.layers.Dense(128, activation="relu"),
     tf.keras.layers.Dense(2)
    ]
)

model.compile(
    optimizer="adam",
    loss=tf.losses.SparseCategoricalCrossentropy(from_logits = True),
    metrics=['accuracy']
)

model.fit(
    train_ds,
    validation_data = test_ds,
    epochs = 10
)